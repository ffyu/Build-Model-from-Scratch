{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anomaly Detection** is an interesting modeling algorithm that can be used to identify rare cases (such as fraud detection). It is normally considered as an unsupervised learning technique. From personal experiecne, I've also appied it for skewed binary classifications and achieved reasonable reasults. Let's now get to the algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial steps of anomaly detection can be summarized as:\n",
    "\n",
    "- Find feature vector $X_1, X_2, ..., X_n$($n$ features) that might be considered to be relevant with the anomaly cases\n",
    "\n",
    "- Fit parameters  $(\\mu_1,\\sigma_1^2), (\\mu_2,\\sigma_2^2), ... (\\mu_n,\\sigma_n^2)$ for Gaussian distributions of all the features. If we have $m$ examples, the fitted parameter can be expressed as:\n",
    "\n",
    "$$\\mu_j=\\frac{1}{m}\\sum_{i=1}^{m}X_j^{(i)}$$\n",
    "\n",
    "$$\\sigma_j^2=\\frac{1}{m}\\sum_{i=1}^{m}(X_j^{(i)}-\\mu_j)^2$$\n",
    "\n",
    "- To test a new example $x$, we can compute $p(x)$:\n",
    "\n",
    "$$p(x)=\\prod_{j=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma_j}exp\\big(-\\frac{(X_j-\\mu_j)^2}{2\\sigma_j^2}\\big)$$\n",
    "\n",
    "- If $p(x)<\\epsilon$, we would consider it as an anomaly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The tricky part is to find the proper value for $\\epsilon$. We can do this by borrowing the idea of cross-validation from supervised learning. That is, if we have a data set containing $m1$ normal examples, and $m2$ abnormal cases. We then divide $m1$ good examples into three parts: say $m11, m12, m13$. We also need to split the $m2$ bad examples into two pieces - $m21, m22$.\n",
    "\n",
    "To learn the parameters of the Gaussian Distributions, we can use all good $m11$ examples.\n",
    "\n",
    "To learn the values of the $\\epsilon$, we can put $m12$ good examples and $m21$ bad examples together, and coded them as 0 and 1. Then we loop through a list of possible $\\epsilon$ values. For each $\\epsilon$, we calculate the corresponding performance of this anomaly detection algorithm (we will use f1-score as evaluation metric to deal with the imbalanced class distribution).\n",
    "\n",
    "In the end, we are left with $m13$ good cases and $m22$ bad cases for test purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####If features are highly correlated, we can apply multivariate Gaussian Distribution instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is still the same. The fitting of distribution parameters and the calcualtion of $p(x)$ are slightly changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean vector and the variance-covariance matrix can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu=\\frac{1}{m}\\sum_{i=1}^{m}X^{(i)}$$\n",
    "\n",
    "$$\\Sigma=\\frac{1}{m}\\sum_{i=1}^{m}(X^{(i)}-\\mu)(X^{(i)}-\\mu)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And $p(x)$ can be calcuated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp\\Bigg(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\Bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The following class AnomalyDetection() covers both non-multivariate and multivariate Gaussian models, and implement both ideas from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class AnomalyDetection():\n",
    "\n",
    "    def __init__(self, multi_variate=False):\n",
    "\n",
    "        # if multi_variate is True, we will use multivariate Gaussian distribution\n",
    "        # to estimate the probabilities\n",
    "        self.multi_variate = multi_variate\n",
    "        self.mu = None\n",
    "        self.sigma2 = None\n",
    "        self.best_epsilon = 0\n",
    "        self.best_f1_score = 0\n",
    "\n",
    "    def _fit_gaussian(self, X_train):\n",
    "\n",
    "        # fit the parameters of the Gaussian Distribution\n",
    "\n",
    "        # if not using the multivariate Gaussian Distribution, we will estimate\n",
    "        # mu and sigma for each single feature distribution separately\n",
    "        if self.multi_variate is False:\n",
    "            self.mu = np.mean(X_train, axis=0)\n",
    "            self.sigma2 = np.var(X_train, axis=0)\n",
    "\n",
    "        # if using the multivariate Gaussian Distribution, we estimate the vector\n",
    "        # of mu and variance/covariance matrix of sigma\n",
    "        else:\n",
    "            m = X_train.shape[0]\n",
    "            self.mu = np.mean(X_train, axis=0)\n",
    "            self.sigma2 = 1.0 / m * (X_train - self.mu).T.dot(X_train - self.mu)\n",
    "\n",
    "    def _prob_calc(self, X):\n",
    "\n",
    "        # helper function to calculate the probability of each instance\n",
    "        # in the cross-validation set\n",
    "        if self.multi_variate is False:\n",
    "            p = np.prod(np.exp(-(X - self.mu) ** 2 / (2.0 * self.sigma2))\n",
    "                        / np.sqrt(2.0 * math.pi * self.sigma2), axis=1)\n",
    "\n",
    "        else:\n",
    "            n = X.shape[1]\n",
    "            p = 1.0 / ((2 * math.pi) ** (n / 2.0) * (np.linalg.det(self.sigma2) ** 0.5)) \\\n",
    "                * np.diag(np.exp(-0.5 * ((X - self.mu).dot(np.linalg.inv(self.sigma2))).dot((X - self.mu).T)))\n",
    "\n",
    "        return p\n",
    "\n",
    "    def _fit_epsilon(self, X_val, y_val):\n",
    "\n",
    "        # this is the second step of model fitting\n",
    "        # the input is the cross-validation set\n",
    "        # the output is the threshold that will maximizes the f1-score\n",
    "        # of the positive class (anomalies) in the CV set\n",
    "\n",
    "        p_val = self._prob_calc(X_val)\n",
    "        p_min = np.array(p_val).min()\n",
    "        p_max =np.array(p_val).max()\n",
    "        step = (p_max - p_min) / 100.0\n",
    "\n",
    "        for epsilon in np.arange(p_min, p_max + step, step):\n",
    "            y_predict = (p_val < epsilon).reshape((len(y_val), 1))\n",
    "\n",
    "            TP = np.sum([1 if y_predict[i] == 1 and y_val[i] == 1 else 0 for i in range(len(y_val))])\n",
    "            PP = np.sum((y_predict == 1))\n",
    "            AP = np.sum((y_val == 1))\n",
    "\n",
    "            if PP == 0 or AP == 0:\n",
    "                continue\n",
    "\n",
    "            precision = float(TP) / PP\n",
    "            recall = float(TP) / AP\n",
    "            f1_score = 2.0 * precision * recall / (precision + recall)\n",
    "\n",
    "            if f1_score > self.best_f1_score:\n",
    "                self.best_f1_score = f1_score\n",
    "                self.best_epsilon = epsilon\n",
    "\n",
    "\n",
    "    def fit(self, X_train, X_val, y_val):\n",
    "\n",
    "        # fit the anomaly detection model\n",
    "        # step 1 - fit mu and sigma based on the training set (all 0s)\n",
    "        # step 2 - fit epsilon based on validation set (0s and 1s)\n",
    "        self._fit_gaussian(X_train)\n",
    "        self._fit_epsilon(X_val, y_val)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "\n",
    "        # predict using fitted model\n",
    "        p_test = self._prob_calc(X_test)\n",
    "        y_test = (p_test < self.best_epsilon).astype(int)\n",
    "\n",
    "        return y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Now let's load the Iris Dataset for a demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset has 150 examples, with three different classes 0, 1, 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L, 4L)\n",
      "(150L,)\n",
      "Number of Classes 0: 50\n",
      "Number of Classes 1: 50\n",
      "Number of Classes 2: 50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "print X.shape\n",
    "print y.shape\n",
    "\n",
    "print \"Number of Classes 0: {}\".format(np.sum(y == 0))\n",
    "print \"Number of Classes 1: {}\".format(np.sum(y == 1))\n",
    "print \"Number of Classes 2: {}\".format(np.sum(y == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just assume class 2 as the anomaly for test purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_new = y\n",
    "y_new[y == 0] = 0\n",
    "y_new[y == 1] = 0\n",
    "y_new[y == 2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 100 normal examples, and 50 abnormal cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes 0: 100\n",
      "Number of Classes 1: 50\n"
     ]
    }
   ],
   "source": [
    "print \"Number of Classes 0: {}\".format(np.sum(y_new == 0))\n",
    "print \"Number of Classes 1: {}\".format(np.sum(y_new == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can split the dataset into train (with all normal examples), validation (normal & abnormal examples), and test (the rest of normal & abnormal examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60L, 4L)\n",
      "(45L, 4L)\n",
      "(45L, 4L)\n"
     ]
    }
   ],
   "source": [
    "X_normal = X[y_new == 0]\n",
    "y_normal = y_new[y_new == 0]\n",
    "X_abnormal = X[y_new == 1]\n",
    "y_abnormal = y_new[y_new == 1]\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_normal_train_val, X_normal_test, y_normal_train_val, y_normal_test = \\\n",
    "    train_test_split(X_normal, y_normal, test_size=0.2, random_state=26)\n",
    "\n",
    "X_normal_train, X_normal_val, y_normal_train, y_normal_val = \\\n",
    "    train_test_split(X_normal_train_val, y_normal_train_val, test_size=0.25, random_state=26)\n",
    "\n",
    "X_abnormal_val, X_abnormal_test, y_abnormal_val, y_abnormal_test = \\\n",
    "    train_test_split(X_abnormal, y_abnormal, test_size=0.5, random_state=26)\n",
    "\n",
    "X_train = X_normal_train\n",
    "y_train = y_normal_train\n",
    "X_val = np.r_[X_normal_val, X_abnormal_val]\n",
    "y_val = np.r_[y_normal_val, y_abnormal_val]\n",
    "X_test = np.r_[X_normal_test, X_abnormal_test]\n",
    "y_test = np.r_[y_normal_test, y_abnormal_test]\n",
    "\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start an **AnmalyDetection()** object, use train and validation data to fit the Gaussian Distribution paramters and find the proper value for $\\epsilon$. With the fitted model, we can predict the anomaly cases on the test dataset and check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Values:      [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n",
      "Predicted Values: [1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1]\n",
      "Prediction Accuracy: 91.11%\n"
     ]
    }
   ],
   "source": [
    "ad = AnomalyDetection(multi_variate=False)\n",
    "ad.fit(X_train, X_val, y_val)\n",
    "y_predict = ad.predict(X_test)\n",
    "print \"True Values:      {}\".format(y_test)\n",
    "print \"Predicted Values: {}\".format(y_predict)\n",
    "print \"Prediction Accuracy: {:.2%}\".format(np.mean((y_predict == y_test).astype(float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, Anomaly Detection is a pretty useful tool for rare case identification. The multivariate Gaussian Distribution is clearly more complex and computationally costly compared to non-multivariate method. It has the advantage of capturing correlations between different features but requires the number of examples larger than number of feature dimensions to avoid matrix singularity issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
