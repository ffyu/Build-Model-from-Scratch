{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression** is perhaps the most straightforward supervised learning technique for regression problems. This notebook will walk through the basic algorithms of the model and build this learning tool in python.\n",
    "\n",
    "1) we can define the notations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***$m$*** - Number of training examples\n",
    "\n",
    "***$n$*** - Number of features\n",
    "\n",
    "***$X$*** - Features\n",
    "\n",
    "***$y$*** - Target\n",
    "\n",
    "***$\\theta$*** - Parameters\n",
    "\n",
    "***$h_\\theta(x)$*** - Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The cost function (witn L2 regularization term) is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}\\Big(h_\\theta(x^{(i)})-y^{(i)}\\Big)^2+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The Gradient is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\Big(h_\\theta(x^{(i)})-y^{(i)}\\Big)X_j^{(i)}+\\frac{\\lambda}{m}\\theta_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the cost function with repect to $\\theta$. Once we know the cost function and the graident, we can apply Gradient Descent or other more advanced optimization algorithms to learn the parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The following class ***LinearRegression()*** builds the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class LinearRegression():\n",
    "\n",
    "    def __init__(self, lamda):\n",
    "\n",
    "        self._lamda = lamda\n",
    "        self._mu = None\n",
    "        self._sigma = None\n",
    "        self._coef = None\n",
    "\n",
    "    def _feature_norm(self, X):\n",
    "\n",
    "        # Normalize all features to expedite the gradient descent process\n",
    "        mu = np.mean(X, axis=0)\n",
    "        sigma = np.std(X, axis=0)\n",
    "        X_norm = (X - mu) / sigma\n",
    "\n",
    "        return X_norm, mu, sigma\n",
    "\n",
    "    def _cost_calc(self, theta, X, y):\n",
    "\n",
    "        # Formulate cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        J = 1.0 / (2 * m) * sum((X.dot(theta) - y)**2) \\\n",
    "            + self._lamda / (2.0 * m) * sum(theta[1:]**2)\n",
    "\n",
    "        return J\n",
    "\n",
    "    def _gradient_calc(self, theta, X, y):\n",
    "\n",
    "        # Formulate the gradient of the cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        grad = np.zeros((n, 1))\n",
    "        grad[0] = 1.0 / m * sum(X.dot(theta) - y)\n",
    "        grad[1:] = 1.0 / m * X[:, 1:].T.dot(X.dot(theta) - y) \\\n",
    "                   + float(self._lamda) / m * theta[1:]\n",
    "\n",
    "        return grad.ravel()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Fit the model\n",
    "        m, n = X.shape\n",
    "        X, self._mu, self._sigma = self._feature_norm(X)\n",
    "        X = np.c_[np.ones((m, 1)), X]\n",
    "        theta = np.zeros(X.shape[1])\n",
    "        result = scipy.optimize.minimize(fun=self._cost_calc, x0=theta, args=(X, y),\n",
    "                                         method='BFGS', jac=self._gradient_calc,\n",
    "                                         options={\"maxiter\": 100, \"disp\": False})\n",
    "        \n",
    "        self._coef = result.x\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # predict with the fitted model\n",
    "        m, n = X.shape\n",
    "        X = np.c_[np.ones((m, 1)), (X - self._mu) / self._sigma]\n",
    "        y_pred = X.dot(self._coef.reshape((n+1, 1)))\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the Boston Housing Data to do a demo.\n",
    "\n",
    "Sklearn.datasets already incorporate this dataset. We can directly load from there.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506L, 13L)\n",
      "(506L,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then randomly split the train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit the model on training set using the **LinearRegression()** class, and then predict on the test set to check the Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 4.557\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(lamda=0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print \"RMSE is {:.4}\".format(np.sqrt(np.mean((y_test.reshape((len(y_test), 1)) - y_pred) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Other things to consider:\n",
    "- L1 (Lasso) Regularization Term that produces sparsity features\n",
    "- Elastic Net that combines L1 (Lasso) and L2 (Ridge) terms\n",
    "- Stochatic Gradient Descent that "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
