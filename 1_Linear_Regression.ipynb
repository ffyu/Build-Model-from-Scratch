{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression** is perhaps the most straightforward supervised learning technique for regression problems. This notebook will walk through the basic algorithms of the model and build this learning tool in python.\n",
    "\n",
    "1) we can define the notations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***$m$*** - Number of training examples\n",
    "\n",
    "***$n$*** - Number of features\n",
    "\n",
    "***$X$*** - Features\n",
    "\n",
    "***$y$*** - Target\n",
    "\n",
    "***$\\theta$*** - Parameters\n",
    "\n",
    "***$h_\\theta(x)$*** - Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The cost function (witn regularization term) is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}\\Big(h_\\theta(x^{(i)})-y^{(i)}\\Big)^2+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The Gradient is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\Big(h_\\theta(x^{(i)})-y^{(i)}\\Big)X_j^{(i)}+\\frac{\\lambda}{m}\\theta_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the cost function with repect to $\\theta$. Once we know the cost function and the graident, we can apply Gradient Descent or other more advanced optimization algorithms to learn the parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The following class ***LinearRegression()*** builds the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class LinearRegression():\n",
    "\n",
    "    def __init__(self, lamda):\n",
    "\n",
    "        self._lamda = lamda\n",
    "        self._coef = None\n",
    "\n",
    "    def _feature_norm(self, X):\n",
    "\n",
    "        # Normalize all features to expedite the gradient descent process\n",
    "        mu = np.mean(X, axis=0)\n",
    "        sigma = np.std(X, axis=0)\n",
    "        X_norm = (X - mu) / sigma\n",
    "\n",
    "        return X_norm, mu, sigma\n",
    "\n",
    "    def _cost_calc(self, theta, X, y):\n",
    "\n",
    "        # Formulate cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        J = 1.0 / (2 * m) * sum((X.dot(theta) - y)**2) \\\n",
    "            + self._lamda / (2.0 * m) * sum(theta[1:]**2)\n",
    "\n",
    "        return J\n",
    "\n",
    "    def _gradient_calc(self, theta, X, y):\n",
    "\n",
    "        # Formulate the gradient of the cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        grad = np.zeros((n, 1))\n",
    "        grad[0] = 1.0 / m * sum(X.dot(theta) - y)\n",
    "        grad[1:] = 1.0 / m * X[:, 1:].T.dot(X.dot(theta) - y) \\\n",
    "                   + float(self._lamda) / m * theta[1:]\n",
    "\n",
    "        return grad.ravel()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Fit the model\n",
    "        m, n = X.shape\n",
    "        X, mu, sigma = self._feature_norm(X)\n",
    "        X = np.c_[np.ones((m, 1)), X]\n",
    "        theta = np.zeros(X.shape[1])\n",
    "        result = scipy.optimize.minimize(fun=self._cost_calc, x0=theta, args=(X, y),\n",
    "                                         method='BFGS', jac=self._gradient_calc,\n",
    "                                         options={\"maxiter\": 100, \"disp\": False})\n",
    "\n",
    "        self._coef = np.hstack((result.x[0], np.array(result.x[1:]) * sigma + mu))\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # predict with the fitted model\n",
    "        m, n = X.shape\n",
    "        X = np.c_[np.ones((m, 1)), X]\n",
    "        y_pred = X.dot(self._coef.reshape((n+1, 1)))\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continued..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
