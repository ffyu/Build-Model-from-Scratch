{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K Nearest Neighbor (KNN)** is a popular non-parametric method. The prediction (for regression/classification) is obtained by looking into the K closest memorized examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The algorithm itself can be summarized into three steps:\n",
    "\n",
    "- Select a positive integer K along with a new example\n",
    "\n",
    "- Select K entries in the training databse which are closest to the new example\n",
    "\n",
    "- For regression problem, we perform an average or weighted average of the response of these closest training examples to make the prediction. For classification scenarios, we do a majority vote within the traning entries to assign the label to the new example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class KNearestNeighbors() implements this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "class KNearestNeighbors():\n",
    "\n",
    "    def __init__(self, k, model_type='regression', weights='uniform'):\n",
    "\n",
    "        # model_type can be either 'classification' or 'regression'\n",
    "        # weights = 'uniform', the K nearest neighbors are equally weighted\n",
    "        # weights = 'distance', the K nearest entries are weighted by inverse of the distance\n",
    "        self.model_type = model_type\n",
    "        self.k = k\n",
    "        self.weights = weights\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def _dist(self, example1, example2):\n",
    "\n",
    "        # calculate euclidean distance between two examples\n",
    "        if len(example1) != len(example2):\n",
    "            print \"Inconsistent Dimension!\"\n",
    "            return\n",
    "\n",
    "        return np.sqrt(sum(np.power(np.array(example1) - np.array(example2), 2)))\n",
    "\n",
    "    def _find_neighbors(self, test_instance):\n",
    "\n",
    "        # find K nearest neighbors for a test instance\n",
    "        # this function return a list of K nearest neighbors for this test instance,\n",
    "        # each element of the list is another list of distance and target\n",
    "        m, n = self.X_train.shape\n",
    "        neighbors = [[self._dist(self.X_train[i, :], test_instance), self.y_train[i]]\n",
    "                     for i in range(m)]\n",
    "        neighbors.sort(key=lambda x: x[0])\n",
    "        return neighbors[:self.k]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # no parameters learning in model fitting process for KNN\n",
    "        # just to store all the training instances\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # predict using KNN algorithm\n",
    "        X = np.array(X)\n",
    "\n",
    "        # if only have one test example to predict\n",
    "        if len(X.shape) == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "\n",
    "        m = X.shape[0]\n",
    "        y_predict = np.zeros((m, 1))\n",
    "\n",
    "        # for regression problems, depending on the weights ('uniform' or 'distance'),\n",
    "        # it will perform average or weighted average based on inverse of distance\n",
    "        if self.model_type == 'regression':\n",
    "            for i in range(m):\n",
    "                distance_mat = np.array(self._find_neighbors(X[i, :]))\n",
    "                if self.weights == 'distance':\n",
    "                    y_predict[i] = np.average(distance_mat[:, 1], weights=1.0/distance_mat[:, 0])\n",
    "                else:\n",
    "                    y_predict[i] = np.average(distance_mat[:, 1])\n",
    "\n",
    "        # for classification, we will apply majority vote for prediction\n",
    "        # it still offer two options in terms of the weights\n",
    "        else:\n",
    "            for i in range(m):\n",
    "                votes = {}\n",
    "                distance_mat = np.array(self._find_neighbors(X[i, :]))\n",
    "                for j in range(self.k):\n",
    "                    if self.weights == 'distance':\n",
    "                        votes[distance_mat[j, 1]] = votes.get(distance_mat[j, 1], 0) + 1.0 / distance_mat[j, 0]\n",
    "                    else:\n",
    "                        votes[distance_mat[j, 1]] = votes.get(distance_mat[j, 1], 0) + 1.0\n",
    "                sorted_votes = sorted(votes.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "                y_predict[i] = sorted_votes[0][0]\n",
    "\n",
    "            y_predict = y_predict.astype(int)\n",
    "\n",
    "        return y_predict.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's first look at a demo for classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Iris data set** with three differe classes will be loaded here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L, 4L)\n",
      "(150L,)\n",
      "Number of Classes: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "print X.shape\n",
    "print y.shape\n",
    "print \"Number of Classes: {}\".format(len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the original data for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define an object of the **KNearestNeighbors()** class to train the model and predict for test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Values:      [1 1 0 0 2 1 0 2 2 2 0 1 1 0 1 0 2 1 2 0 0 2 2 2 2 2 1 0 0 2]\n",
      "Predicted Values: [1 1 0 0 2 1 0 2 2 2 0 1 1 0 1 0 2 1 2 0 0 2 2 2 2 1 1 0 0 2]\n",
      "Prediction Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "knn = KNearestNeighbors(k=3, model_type='classification', weights='uniform')\n",
    "knn = knn.fit(X_train, y_train)\n",
    "y_predict = knn.predict(X_test)\n",
    "print \"True Values:      {}\".format(y_test)\n",
    "print \"Predicted Values: {}\".format(y_predict)\n",
    "print \"Prediction Accuracy: {:.2%}\".format(np.mean((y_predict == y_test).astype(float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Let's then look at a demo for regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston housing data will be loaded here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506L, 13L)\n",
      "(506L,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's split the entire data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.025, random_state=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will utilize the **KNearestNeighbors()** class again to train the model and predict for test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Values:\n",
      "[19.2, 32.0, 14.3, 13.2, 20.0, 30.7, 23.1, 22.6, 20.9, 13.4, 14.6, 17.8, 16.7]\n",
      "Predicted Values:\n",
      "[20.0, 25.2, 18.3, 19.0, 23.5, 30.3, 12.9, 25.7, 24.8, 12.9, 19.9, 18.0, 18.1]\n",
      "RMSE is 7.644\n"
     ]
    }
   ],
   "source": [
    "knn = KNearestNeighbors(k=20, model_type='regression', weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "y_predict = knn.predict(X_test)\n",
    "print \"True Values:\\n{}\".format([round(elem, 1) for elem in y_test])\n",
    "print \"Predicted Values:\\n{}\".format([round(elem, 1) for elem in y_predict])\n",
    "print \"RMSE is {:.4}\".format(np.sqrt(np.mean((y_test.reshape((len(y_test), 1)) - y_predict) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the good side about KNN is that we don't have to tune complex parameters to build the model. However, when the data size is large, the computation cost of finding the nearest neighbors for all test examples is high.  Also, the performance of the model relies on the dimension of features. There is always the thing called the ***\"Curse of Dimensionality\"***. With high dimension, the nearest neighbors based on distance measures can be actually very far away. Thus, sometimes it is helpful to run PCA to reduce dimension before running KNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
