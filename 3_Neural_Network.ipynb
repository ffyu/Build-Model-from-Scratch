{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Networks** is a widely used tool in many fields. It incorporates a collection of neurons and connects them into a network to learn and predict patterns. This notebook will walk through the basics of Artificial Neural Network, and built a typical three layer network in Python from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, let's discuss about the architecture of the Neural Networks. Below is a diagram depicting a three layer network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Pictures/NN_1.png\" width=450 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks usually have three types of layers. The first layer is called the **input layer**. The last layer is named the **output layer**, which produces values computed by the hypothesis. The layers in the middle are referred to as the **hidden layers**. We can identify more than 1 hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **nodes** in the input layer basically represents the features of the model, i.e. $X_1, X_2,..., X_n$. It is also good to add a **bias node**, $X_0$, to the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each node in the hidden layers, it 1)takes the inputs from either the input layer or the previous hidden layer, 2)computes the output value based on weights and activation function associated with the node, and 3)sends output to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes in the outpyut layer calculate final values for the hypothesis. In a classification problem, the number of nodes in the output layer is equal to number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's define some notations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***$S_l$***: total number of nodes in layer $l$\n",
    "\n",
    "***$\\Theta^{(l)}$***: weight matrix controlling the mapping from layer $l$ to layer $l+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write down the elements of this metrix $\\Theta^{(l)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix}\n",
    "  \\Theta_{10}^{(l)} & \\Theta_{11}^{(l)} & \\dots & \\Theta_{1S_l}^{(l)} \\\\\n",
    "  \\Theta_{20}^{(l)} & \\Theta_{21}^{(l)} & \\dots & \\Theta_{2S_l}^{(l)} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\Theta_{S_{l+1}0}^{(l)} & \\Theta_{S_{l+1}1}^{(l)} & \\dots & \\Theta_{S_{l+1}S_l}^{(l)}\n",
    "  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, element $\\Theta_{ji}^{(l)}$ stands for the weight (parameter) from $i^{th}$ node in layer $l$ to $j^{th}$ node in layer $l+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define the **\"activation\"** of the node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***$a_i^{(l)}$***: activation of node $i$ in layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the **\"activation\"** means the computed value output by each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the activation of each node, we need three things:\n",
    "\n",
    "- The inputs\n",
    "- The weights\n",
    "- The activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a random node $j$ in layer $l+1$, we can plot this individual node and its input/ouput wires:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Pictures/NN_2.png\" width=270 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define the linear combination of the inputs and weights to be $z_j^{(l+1)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_j^{(l+1)}=\\sum_{i=0}^{S_l}a_i^{(l)}\\Theta_{ji}^{(l)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $a_0^{(l)}$ here is the added bias unit (which takes the values of 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the activation $a_j^{(l+1)}$ can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a_j^{(l+1)}=g\\big(z_j^{(l+1)}\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $g(z)$ is called the activation function. It can take various forms. We will use the same Sigmoid Function as in the Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Now we can put all these notations together in matrix (vectorized) form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{(1)}=\\begin{bmatrix}\n",
    "           X_0 \\\\\n",
    "           X_1 \\\\\n",
    "           \\vdots \\\\\n",
    "           X_n\n",
    "          \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a three layer neural network, we can calculate the activation of each node as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{(2)}=\\Theta^{(1)}a^{(1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{(2)}=g\\big(z^{(2)}\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{(2)}=[a_0^{(2)};\\ a^{(2)}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{(3)}=\\Theta^{(2)}a^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{(3)}=g\\big(z^{(3)}\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a three layer network, the third layer is aslo the output layer. We can define model hypothesis output as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_\\Theta(X)=a^{(3)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process of calculating the activation for each node is also known as **\"forward propagation\"**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class **ThreeLayerNeuralNetwork()** implement the neural network with input, hidden, and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class ThreeLayerNeuralNetwork():\n",
    "\n",
    "    def __init__(self, layer1_size, layer2_size, layer3_size, lamda):\n",
    "\n",
    "        self.layer1_size = layer1_size\n",
    "        self.layer2_size = layer2_size\n",
    "        self.layer3_size = layer3_size\n",
    "        self.lamda = lamda\n",
    "        self._mu = None\n",
    "        self._sigma = None\n",
    "        self._params = None\n",
    "\n",
    "    def _feature_norm(self, X):\n",
    "\n",
    "        # it is good practice to normalize features before training a neural network\n",
    "        mu = np.mean(X, axis=0)\n",
    "        sigma = np.std(X, axis=0)\n",
    "        X_norm = (X - mu) / sigma\n",
    "\n",
    "        return X_norm, mu, sigma\n",
    "\n",
    "    def _param_reshape(self, params):\n",
    "\n",
    "        # the input of this function is 1d array with all the parameters for all layers\n",
    "        # the output will be two 2d array with parameters from layer1 to layer2, \n",
    "        # and layer 2 to layer 3\n",
    "        theta1_num = (self.layer1_size + 1) * self.layer2_size\n",
    "        theta1_size = (self.layer2_size, self.layer1_size + 1)\n",
    "        theta2_size = (self.layer3_size, self.layer2_size + 1)\n",
    "        theta1 = params[:theta1_num].reshape(theta1_size)\n",
    "        theta2 = params[theta1_num:].reshape(theta2_size)\n",
    "        return theta1, theta2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "\n",
    "        # formulate sigmoid function\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def _sigmoid_grad(self, z):\n",
    "\n",
    "        # formulate gradient of the sigmoid function\n",
    "        g = self._sigmoid(z)\n",
    "        return g * (1 - g)\n",
    "\n",
    "    def _label_expansion(self, y):\n",
    "\n",
    "        # the input of this function is the label with one single value for each example\n",
    "        # the output is a matrix with each column corresponding to a training example\n",
    "        # the size of the output matrix is (num_classes, num_examples)\n",
    "        m = len(y)\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_mat = np.zeros((num_classes, m))\n",
    "        for i in range(m):\n",
    "            y_mat[y[i], i] = 1\n",
    "\n",
    "        return y_mat\n",
    "\n",
    "    def _forward_prop(self, theta1, theta2, X):\n",
    "\n",
    "        # forward propagation given the current weights\n",
    "        a1 = np.r_[np.ones((1, X.shape[0])), X.T]\n",
    "        z2 = theta1.dot(a1)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = np.r_[np.ones((1, a2.shape[1])), a2]\n",
    "        z3 = theta2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "\n",
    "        return a1, a2, a3, z2, z3\n",
    "\n",
    "    def _back_prop(self, theta2, a3, z2, y):\n",
    "\n",
    "        # back propagation\n",
    "        m = len(y)\n",
    "        y_mat = self._label_expansion(y)\n",
    "        sigma3 = a3 - y_mat\n",
    "        sigma2 = theta2.T.dot(sigma3) * self._sigmoid_grad(np.r_[np.ones((1, m)), z2])\n",
    "        sigma2 = sigma2[1:, :]\n",
    "\n",
    "        return sigma2, sigma3\n",
    "\n",
    "    def _cost_calc(self, params, X, y):\n",
    "\n",
    "        # formulate the cost function\n",
    "        m = X.shape[0]\n",
    "        theta1, theta2 = self._param_reshape(params)\n",
    "        y_mat = self._label_expansion(y)\n",
    "        _, _, a3, _, _ = self._forward_prop(theta1, theta2, X)\n",
    "        term1 = -1.0 / m * sum((y_mat * np.log(a3) + (1 - y_mat) * np.log(1 - a3)).ravel())\n",
    "        term2 = 1.0 * self.lamda / (2.0 * m) * (sum((theta1[:, 1:] ** 2).ravel()) + \n",
    "                                                sum((theta2[:, 1:] ** 2).ravel()))\n",
    "\n",
    "        return term1 + term2\n",
    "\n",
    "    def _grad_calc(self, params, X, y):\n",
    "\n",
    "        # formulate gradient\n",
    "        m = X.shape[0]\n",
    "        theta1, theta2 = self._param_reshape(params)\n",
    "        a1, a2, a3, z2, _ = self._forward_prop(theta1, theta2, X)\n",
    "        sigma2, sigma3 = self._back_prop(theta2, a3, z2, y)\n",
    "        grad1 = sigma2.dot(a1.T) / (1.0 * m)\n",
    "        grad2 = sigma3.dot(a2.T) / (1.0 * m)\n",
    "        grad1[:, 1:] = grad1[:, 1:] + theta1[:, 1:] * float(self.lamda) / m\n",
    "        grad2[:, 1:] = grad2[:, 1:] + theta2[:, 1:] * float(self.lamda) / m\n",
    "\n",
    "        return np.r_[grad1.ravel(), grad2.ravel()]\n",
    "\n",
    "    def _rand_initialize_param(self, layer_in, layer_out):\n",
    "\n",
    "        # initialize parameters (weights) for neural networks\n",
    "        epsilon = 0.12\n",
    "        num = layer_out * (layer_in + 1)\n",
    "        params = np.array([np.random.random() * 2 * epsilon - epsilon for i in range(num)])\n",
    "        params = params.reshape((layer_out, layer_in + 1))\n",
    "\n",
    "        return params\n",
    "\n",
    "    def _numeric_grad_calc(self, params, X, y):\n",
    "\n",
    "        # calculate numerical gradient\n",
    "        # used to check the gradient\n",
    "        # plz turn it off after confirm gradient is correct\n",
    "        numeric_grad = np.zeros(params.shape)\n",
    "        perturb = np.zeros(params.shape)\n",
    "        e = 1e-4\n",
    "        for i in range(numeric_grad.shape[0]):\n",
    "            perturb[i] = e\n",
    "            loss1 = self._cost_calc(params - perturb, X, y)\n",
    "            loss2 = self._cost_calc(params + perturb, X, y)\n",
    "            numeric_grad[i] = (loss2 - loss1) / (2.0 * e)\n",
    "            perturb[i] = 0\n",
    "\n",
    "        return numeric_grad\n",
    "\n",
    "    def _debug_matrix_init(self, num_row, num_col):\n",
    "\n",
    "        # used to randomly initialize X and y for debug purposes\n",
    "        mat = np.array([np.sin(x) / 10 for x in range(1, num_row * num_col + 1)])\n",
    "        mat = mat.reshape(num_row, num_col)\n",
    "\n",
    "        return mat\n",
    "\n",
    "    def grad_check(self):\n",
    "\n",
    "        # check the correctness of the gradient calculation\n",
    "        # use 10 examples for debug purpose only\n",
    "        m = 10\n",
    "        theta1 = self._rand_initialize_param(self.layer1_size, self.layer2_size)\n",
    "        theta2 = self._rand_initialize_param(self.layer2_size, self.layer3_size)\n",
    "        X = self._debug_matrix_init(m, self.layer1_size)\n",
    "        tempY = np.array([x for x in range(1, m + 1)])\n",
    "        y = 1 + np.mod(tempY, self.layer3_size)\n",
    "        y = y.reshape((m, 1))\n",
    "        params = np.r_[theta1.ravel(), theta2.ravel()]\n",
    "        grad = self._grad_calc(params, X, y)\n",
    "        numeric_grad = self._numeric_grad_calc(params, X, y)\n",
    "\n",
    "        # if gradient calculation is correct, \n",
    "        # the difference between gradient and numerical gradient should be small\n",
    "        diff = np.linalg.norm(numeric_grad - grad) / np.linalg.norm(numeric_grad + grad)\n",
    "        print diff\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # train the model\n",
    "        X, self._mu, self._sigma = self._feature_norm(X)\n",
    "        theta1 = self._rand_initialize_param(self.layer1_size, self.layer2_size)\n",
    "        theta2 = self._rand_initialize_param(self.layer2_size, self.layer3_size)\n",
    "        init_params = np.r_[theta1.ravel(), theta2.ravel()]\n",
    "        result = scipy.optimize.minimize(fun=self._cost_calc, x0=init_params, args=(X, y),\n",
    "                                         method='BFGS', jac=self._grad_calc,\n",
    "                                         options={\"maxiter\": 200, \"disp\": False})\n",
    "        self._params = result.x\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # predict with the trained model\n",
    "        theta1, theta2 = self._param_reshape(self._params)\n",
    "        X = (X - self._mu) / self._sigma\n",
    "\n",
    "        # case 1: predict for a single example\n",
    "        if len(X.shape) ==1:\n",
    "            a1 = np.r_[np.ones((1, 1)), X.reshape(X.shape[0], 1)]\n",
    "            a2 = self._sigmoid(theta1.dot(a1))\n",
    "            a2 = np.r_[np.ones((1, 1)), a2]\n",
    "            a3 = self._sigmoid(theta2.dot(a2))\n",
    "\n",
    "            return np.argmax(a3)\n",
    "\n",
    "        # case 2: predict for multiple examples\n",
    "        else:\n",
    "            _, _, a3, _, _ = self._forward_prop(theta1, theta2, X)\n",
    "\n",
    "            return np.argmax(a3, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data for demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L, 4L)\n",
      "(150L,)\n",
      "Number of Classes: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "print X.shape\n",
    "print y.shape\n",
    "print \"Number of Classes: {}\".format(len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.24, random_state=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network with 10 nodes in the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Values:      [1 1 0 0 2 1 0 2 2 2 0 1 1 0 1 0 2 1 2 0 0 2 2 2 2 2 1 0 0 2 1 1 1 2 0 0]\n",
      "Predicted Values: [1 1 0 0 2 1 0 2 2 2 0 1 1 0 1 0 2 1 2 0 0 2 2 2 2 1 1 0 0 2 1 1 1 2 0 0]\n",
      "Prediction Accuracy: 97.22%\n"
     ]
    }
   ],
   "source": [
    "nn = ThreeLayerNeuralNetwork(layer1_size=X.shape[1], layer2_size=10, layer3_size=len(np.unique(y)), lamda=1)\n",
    "nn = nn.fit(X_train, y_train)\n",
    "y_predict = nn.predict(X_test)\n",
    "print \"True Values:      {}\".format(y_test)\n",
    "print \"Predicted Values: {}\".format(y_predict)\n",
    "print \"Prediction Accuracy: {:.2%}\".format(np.mean((y_predict == y_test).astype(float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things to consider:\n",
    "\n",
    "To be continued..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
