{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression** is the basic learning algorithm for classification problems. It is simple but elegant in its form. This notebook will cover the algorithms of Logistic Regression and build it in Python from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We start from the binary classifications####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) let's first define the notations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***$m$*** - Number of training examples\n",
    "\n",
    "***$n$*** - Number of features\n",
    "\n",
    "***$X$*** - Features\n",
    "\n",
    "***$y$*** - Target (discrete)\n",
    "\n",
    "***$(X^{(i)}$,$y^{(i)})$*** - $i^{th}$ taining example\n",
    "\n",
    "***$\\theta: (\\theta_1,\\theta_2,...,\\theta_n)^T$*** - model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The hypothesis of the Logistic Regression is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_\\theta(X) = g(\\theta^TX)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Where\\;\\; g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Here $g(z)$ is called the Sigmoid Function. We will use it for the activation function of neurons later in the Neural Network as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The log-loss is then used to define the cost function of the Logistic Regression (L2 Regularization Term):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}\\Big(y^{(i)}log\\big(h_\\theta(X^{(i)})\\big)+(1-y^{(i)})log\\big(1-h_\\theta(X^{(i)})\\big)\\Big)+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) To minimize cost function with respect to the parameters, we can calculate the gradient (for $j>=1$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\Big(h\\big(\\theta(X_{(i)})\\big)-y^{(i)}\\Big)X_j^{(i)}+\\frac{\\lambda}{m}\\theta_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The following class BinaryLogisticRegression() builds the binary classification model:####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class BinaryLogisticRegression():\n",
    "\n",
    "    def __init__(self, lamda):\n",
    "\n",
    "        self._lamda = lamda\n",
    "\n",
    "    # Define class variables\n",
    "    _mu = None\n",
    "    _sigma = None\n",
    "    _coef = None\n",
    "\n",
    "    def _feature_norm(self, X):\n",
    "\n",
    "        # Normalize all features to expedite the gradient descent process\n",
    "        mu = np.mean(X, axis=0)\n",
    "        sigma = np.std(X, axis=0)\n",
    "        X_norm = (X - mu) / sigma\n",
    "\n",
    "        return X_norm, mu, sigma\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "\n",
    "        # Formulate sigmoid function\n",
    "        return 1.0 / (1.0 + np.exp(-1.0 * z))\n",
    "\n",
    "    def _cost_calc(self, theta, X, y):\n",
    "\n",
    "        # Formulate cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        z = X.dot(theta)\n",
    "        h_x = self._sigmoid(z)\n",
    "        J = -1.0 / m * (y.T.dot(np.log(h_x)) + (1 - y).T.dot(np.log(1 - h_x))) \\\n",
    "            + self._lamda / (2.0 * m) * sum(theta[1:]**2)\n",
    "\n",
    "        return J.ravel()\n",
    "\n",
    "    def _gradient_calc(self, theta, X, y):\n",
    "\n",
    "        # Formulate the gradient of the cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        z = X.dot(theta)\n",
    "        h_x = self._sigmoid(z)\n",
    "        grad = np.zeros((n, 1))\n",
    "        grad[0] = 1.0 / m * sum(h_x - y)\n",
    "        grad[1:] = 1.0 / m * X[:, 1:].T.dot(h_x - y) + float(self._lamda) / m * theta[1:]\n",
    "\n",
    "        return grad.ravel()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Fit the model\n",
    "        m, n = X.shape\n",
    "        X, self._mu, self._sigma = self._feature_norm(X)\n",
    "        X = np.c_[np.ones((m, 1)), X]\n",
    "        theta = np.zeros(X.shape[1])\n",
    "        result = scipy.optimize.minimize(fun=self._cost_calc, x0=theta, args=(X, y),\n",
    "                                         method='BFGS', jac=self._gradient_calc,\n",
    "                                         options={\"maxiter\": 100, \"disp\": False})\n",
    "\n",
    "        self._coef = result.x\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "\n",
    "        # predict probabilities with the fitted model\n",
    "        m, n = X.shape\n",
    "        X = np.c_[np.ones((m, 1)), (X - self._mu) / self._sigma]\n",
    "        y_prob = self._sigmoid(X.dot(self._coef.reshape((n+1, 1))))\n",
    "\n",
    "        return y_prob.ravel()\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # predict with the fitted model\n",
    "        p = self.predict_prob(X)\n",
    "        y_predict = np.copy(p)\n",
    "        y_predict[p > 0.5] = 1\n",
    "        y_predict[p <= 0.5] = 0\n",
    "\n",
    "        return y_predict.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Now let's focus on multi-class classification####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the **\"one-versus-all\"** scheme to achive the goal. To be specific, for each class, we code it as the positive class (1), while the rest of classes are coded as the negative class (0). We then apply Binary Logistic Regression and predict the probabilities of being positive for each class against all others. The class with the highest probability of being positive will be chosen as the final prediction of the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The following class MultiClassLogisticRegression() implements this idea.####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiClassLogisticRegression():\n",
    "\n",
    "    def __init__(self, lamda):\n",
    "\n",
    "        self._lamda = lamda\n",
    "\n",
    "    # Define class variables\n",
    "    _values = None\n",
    "    _n_samples = None\n",
    "    _n_classes = None\n",
    "    _lr_list = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # fit the model\n",
    "        self._values = np.unique(y)\n",
    "        self._n_samples = X.shape[0]\n",
    "        self._n_classes = len(self._values)\n",
    "        y_copy = np.array(y)\n",
    "\n",
    "        for value in self._values:\n",
    "            idx = (y == value)\n",
    "            y_copy[idx] = 1\n",
    "            y_copy[~idx] = 0\n",
    "            lr = BinaryLogisticRegression(lamda=self._lamda)\n",
    "            self._lr_list.append(lr.fit(X, y_copy))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "\n",
    "        # predict the probabilities for each class\n",
    "        y_prob_ini = np.empty((X.shape[0], self._n_classes))\n",
    "        for k, lr in enumerate(self._lr_list):\n",
    "            y_prob_ini[:, k] = lr.predict_prob(X)\n",
    "\n",
    "        row_sums = y_prob_ini.sum(axis=1)\n",
    "        y_prob = y_prob_ini / row_sums[:, np.newaxis]\n",
    "        return y_prob\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # predict with one_versus_all scheme\n",
    "        p = self.predict_prob(X)\n",
    "        y_predict_idx = np.argmax(p, axis=1)\n",
    "        y_predict = np.array([self._values[i] for i in y_predict_idx])\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to test a data set with the learning tool we just built. \n",
    "\n",
    "We've loaded the Iris data set from the Scikit-Learn Module. It has 4 features and 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L, 4L)\n",
      "(150L,)\n",
      "Number of Classes: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "print X.shape\n",
    "print y.shape\n",
    "print \"Number of Classes: {}\".format(len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the original data for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multi-class problem. We then use the defined class ***MultiClassLogisticRegression()*** to train the model and predict the results for test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Values:      [1 1 0 0 2 1 0 2 2 2 0 1 1 0 1 0 2 1 2 0 0 2 2 2 2 2 1 0 0 2]\n",
      "Predicted Values: [1 1 0 0 2 1 0 2 2 2 0 1 1 0 1 0 2 1 2 0 0 2 2 2 2 1 1 0 0 2]\n",
      "Prediction Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "lr = MultiClassLogisticRegression(lamda=1)\n",
    "lr = lr.fit(X_train, y_train)\n",
    "y_predict = lr.predict(X_test)\n",
    "\n",
    "print \"True Values:      {}\".format(y_test)\n",
    "print \"Predicted Values: {}\".format(y_predict)\n",
    "print \"Prediction Accuracy: {:.2%}\".format(np.mean((y_predict == y_test).astype(float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Other things to consider:***\n",
    "- Elastic Net (L1 & L2 Regularization)\n",
    "- If class distribution is skewed, re-sampling or cost-sentitive learning should be applied\n",
    "- Evaluation metrics are not just the prediction accuracy. We should also look at precision, recall, f-score, ROC curve to have a comprehensive understanding of how the classifier performs\n",
    "- Logitic Regression is considered as linear model. It can be pretty useful in the second level with [model stacking](http://www.researchgate.net/publication/222467943_Stacked_Generalization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
